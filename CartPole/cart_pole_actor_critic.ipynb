{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import models\n",
    "import policy_gradient_methods\n",
    "import utils\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "ENVIRONMENT_ID = \"CartPole-v1\"\n",
    "NUM_RUNS_PER_METHOD = 9\n",
    "NUM_EPISODES_PER_RUN = 1000\n",
    "GRAD_CLIP_VALUE = 10\n",
    "GAMMA = 0.99\n",
    "NN_HIDDEN_LAYER_SIZES = [8, 4]\n",
    "RNG_SEED = 7\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"PyTorch device:\", device)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step Actor-Critic\n",
    "\n",
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episodic_one_step_actor_critic(\n",
    "        env, policy_model, value_model, policy_optimizer, value_optimizer, device, rng_seed, num_episodes, gamma, grad_clip_value=None):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        # Set seed only one time per training run. For more info, see https://gymnasium.farama.org/api/env/.\n",
    "        observation, info = env.reset(seed=rng_seed if episode == 0 else None)\n",
    "        state = torch.tensor(observation, device=device)\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        I = 1\n",
    "        G = 0\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                action_probabilities = policy_model(state)\n",
    "            action = policy_gradient_methods.select_action_softmax(action_probabilities)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = torch.tensor(observation, device=device)\n",
    "            with torch.no_grad():\n",
    "                state_value = value_model(state)\n",
    "                next_state_value = value_model(next_state)\n",
    "            delta = reward + gamma * next_state_value - state_value\n",
    "            value_loss = -torch.sum(delta * value_model(state))\n",
    "            policy_gradient_methods.update_model(value_loss, value_optimizer, value_model, grad_clip_value)\n",
    "            policy_loss = -torch.sum(I * delta * torch.log(policy_model(state)[action]))\n",
    "            policy_gradient_methods.update_model(policy_loss, policy_optimizer, policy_model, grad_clip_value)\n",
    "            I *= gamma\n",
    "            state = next_state\n",
    "            G += reward\n",
    "        returns.append(G)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started run 1/9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "select_action_softmax() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\c\\Source\\rl-gymnasium-training\\CartPole\\cart_pole_actor_critic.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     policy_optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(policy_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     value_optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(value_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     returns \u001b[39m=\u001b[39m train_episodic_one_step_actor_critic(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         env, policy_model, value_model, policy_optimizer, value_optimizer, device,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         RNG_SEED, NUM_EPISODES_PER_RUN, GAMMA, GRAD_CLIP_VALUE\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     returns_per_run\u001b[39m.\u001b[39mappend(returns)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32mc:\\Users\\c\\Source\\rl-gymnasium-training\\CartPole\\cart_pole_actor_critic.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m G \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m (terminated \u001b[39mor\u001b[39;00m truncated):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     action \u001b[39m=\u001b[39m policy_gradient_methods\u001b[39m.\u001b[39mselect_action_softmax(policy_model, state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/c/Source/rl-gymnasium-training/CartPole/cart_pole_actor_critic.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     next_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(observation, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[1;31mTypeError\u001b[0m: select_action_softmax() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "returns_per_run = []\n",
    "env = gym.make(ENVIRONMENT_ID)\n",
    "for run in range(NUM_RUNS_PER_METHOD):\n",
    "    print(f\"Started run {run + 1}/{NUM_RUNS_PER_METHOD}\")\n",
    "    policy_model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0], env.action_space.n),\n",
    "        torch.nn.Softmax(dim=0)\n",
    "    ).to(device)\n",
    "    value_model = torch.nn.Linear(env.observation_space.shape[0], 1).to(device)\n",
    "    policy_optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01)\n",
    "    value_optimizer = torch.optim.SGD(value_model.parameters(), lr=0.01)\n",
    "    returns = train_episodic_one_step_actor_critic(\n",
    "        env, policy_model, value_model, policy_optimizer, value_optimizer, device,\n",
    "        RNG_SEED, NUM_EPISODES_PER_RUN, GAMMA, GRAD_CLIP_VALUE\n",
    "    )\n",
    "    returns_per_run.append(returns)\n",
    "env.close()\n",
    "utils.plot_returns_multiple_runs(returns_per_run, env.spec.reward_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
