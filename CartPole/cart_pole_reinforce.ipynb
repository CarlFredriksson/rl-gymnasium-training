{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import models\n",
    "import policy_gradient_methods\n",
    "import utils\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "ENVIRONMENT_ID = \"CartPole-v1\"\n",
    "NUM_EPISODES = 10000\n",
    "GRAD_CLIP_VALUE = 100\n",
    "GAMMA = 0.9\n",
    "NN_HIDDEN_LAYER_SIZES = [8, 4]\n",
    "RNG_SEED = 7\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"PyTorch device:\", device)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_softmax(model, state):\n",
    "    with torch.no_grad():\n",
    "        return torch.distributions.categorical.Categorical(model(state)).sample().item()\n",
    "\n",
    "def train_episodic_reinforce(env, policy_model, loss_func, optimizer, device, rng_seed, num_episodes, gamma, grad_clip_value=None):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        # Initiate episode\n",
    "        # Set seed only one time per training run. For more info, see https://gymnasium.farama.org/api/env/.\n",
    "        seed = rng_seed if episode == 0 else None\n",
    "        observation, info = env.reset(seed=seed)\n",
    "        state = torch.tensor(observation, device=device)\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        G = 0\n",
    "        states = [state]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Generate episode\n",
    "        while not (terminated or truncated):\n",
    "            action = select_action_softmax(policy_model, state)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = torch.tensor(observation, device=device)\n",
    "            states.append(next_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            G += reward\n",
    "            state = next_state\n",
    "        returns.append(G)\n",
    "\n",
    "        # Learn from episode\n",
    "\n",
    "        \n",
    "    return returns, states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6210, 0.3790])\n",
      "0\n",
      "returns: [15.0]\n",
      "states: [tensor([ 0.0125,  0.0397,  0.0276, -0.0275]), tensor([ 0.0133, -0.1558,  0.0270,  0.2738]), tensor([ 0.0102, -0.3513,  0.0325,  0.5749]), tensor([ 0.0032, -0.1566,  0.0440,  0.2926]), tensor([3.0043e-05, 3.7838e-02, 4.9843e-02, 1.4091e-02]), tensor([ 0.0008, -0.1580,  0.0501,  0.3221]), tensor([-0.0024, -0.3538,  0.0566,  0.6301]), tensor([-0.0094, -0.5496,  0.0692,  0.9401]), tensor([-0.0204, -0.3555,  0.0880,  0.6699]), tensor([-0.0276, -0.5517,  0.1014,  0.9889]), tensor([-0.0386, -0.3581,  0.1211,  0.7297]), tensor([-0.0457, -0.5547,  0.1357,  1.0580]), tensor([-0.0568, -0.7513,  0.1569,  1.3900]), tensor([-0.0719, -0.5584,  0.1847,  1.1502]), tensor([-0.0830, -0.7554,  0.2077,  1.4946]), tensor([-0.0981, -0.5634,  0.2376,  1.2733])]\n",
      "16\n",
      "actions: [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]\n",
      "15\n",
      "rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENVIRONMENT_ID)\n",
    "env.action_space.seed(RNG_SEED)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(env.observation_space.shape[0], env.action_space.n).to(device),\n",
    "    torch.nn.Softmax(dim=0)\n",
    ")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "observation, info = env.reset(seed=RNG_SEED)\n",
    "state = torch.tensor(observation, device=device)\n",
    "action = select_action_softmax(model, state)\n",
    "with torch.no_grad():\n",
    "    print(model(state))\n",
    "print(action)\n",
    "returns, states, actions, rewards = train_episodic_reinforce(\n",
    "    env, model, loss_func, optimizer, device, RNG_SEED, 1, GAMMA, GRAD_CLIP_VALUE\n",
    ")\n",
    "print(\"returns:\", returns)\n",
    "print(\"states:\", states)\n",
    "print(len(states))\n",
    "print(\"actions:\", actions)\n",
    "print(len(actions))\n",
    "print(\"rewards:\", rewards)\n",
    "print(len(rewards))\n",
    "env.close()\n",
    "#utils.plot_returns(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE with baseline\n",
    "\n",
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
