{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* Use whole episode as batch when learning?\n",
    "* Try without gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import models\n",
    "import policy_gradient_methods\n",
    "import utils\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "ENVIRONMENT_ID = \"CartPole-v1\"\n",
    "NUM_EPISODES = 10000\n",
    "GRAD_CLIP_VALUE = 100\n",
    "GAMMA = 0.9\n",
    "NN_HIDDEN_LAYER_SIZES = [16, 16]\n",
    "RNG_SEED = 7\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"PyTorch device:\", device)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_softmax(model, state):\n",
    "    with torch.no_grad():\n",
    "        return torch.distributions.categorical.Categorical(model(state)).sample().item()\n",
    "\n",
    "def train_episodic_reinforce(env, policy_model, optimizer, device, rng_seed, num_episodes, gamma, grad_clip_value=None):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        # Initiate episode\n",
    "        # Set seed only one time per training run. For more info, see https://gymnasium.farama.org/api/env/.\n",
    "        seed = rng_seed if episode == 0 else None\n",
    "        observation, info = env.reset(seed=seed)\n",
    "        state = torch.tensor(observation, device=device)\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        G = 0\n",
    "        states = [state]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # Generate episode\n",
    "        while not (terminated or truncated):\n",
    "            action = select_action_softmax(policy_model, state)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = torch.tensor(observation, device=device)\n",
    "            states.append(next_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            G += reward\n",
    "            state = next_state\n",
    "        returns.append(G)\n",
    "\n",
    "        # Learn from episode\n",
    "        G = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = gamma * G + rewards[t]\n",
    "            loss = -gamma**t * G * torch.log(policy_model(states[t])[actions[t]])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if grad_clip_value != None:\n",
    "                torch.nn.utils.clip_grad_value_(policy_model.parameters(), grad_clip_value)\n",
    "            optimizer.step()\n",
    "        \n",
    "    return returns, states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT_ID)\n",
    "policy_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(env.observation_space.shape[0], env.action_space.n),\n",
    "    torch.nn.Softmax(dim=0)\n",
    ").to(device)\n",
    "optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01)\n",
    "returns, states, actions, rewards = train_episodic_reinforce(\n",
    "    env, policy_model, optimizer, device, RNG_SEED, NUM_EPISODES, GAMMA, GRAD_CLIP_VALUE\n",
    ")\n",
    "utils.plot_returns(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT_ID)\n",
    "policy_model = models.create_simple_nn(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.n,\n",
    "    NN_HIDDEN_LAYER_SIZES,\n",
    "    output_activation=\"softmax\"\n",
    ").to(device)\n",
    "#optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=0.001, amsgrad=True)\n",
    "returns, states, actions, rewards = train_episodic_reinforce(\n",
    "    env, policy_model, optimizer, device, RNG_SEED, NUM_EPISODES, GAMMA, GRAD_CLIP_VALUE\n",
    ")\n",
    "utils.plot_returns(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE with baseline\n",
    "\n",
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
